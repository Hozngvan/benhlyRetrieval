import streamlit as st
import google.generativeai as genai
from dotenv import load_dotenv
import os
from retrieval import load_data, search_symptoms

# Load environment variables
load_dotenv()

# Configure Gemini API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

@st.cache_resource
def load_model():
    # Initialize Gemini model
    model = genai.GenerativeModel('gemini-1.5-flash')
    return model

def generate_response(query, results, model):
    # Create prompt template
    template = """Báº¡n lÃ  má»™t trá»£ lÃ½ y táº¿ thÃ´ng minh. HÃ£y sá»­ dá»¥ng thÃ´ng tin cá»§a query vÃ  context Ä‘á»ƒ xÃ¡c Ä‘á»‹nh bá»‡nh mÃ  ngÆ°á»i há»i cÃ³ thá»ƒ Ä‘ang máº¯c pháº£i. 
    CÃ³ thá»ƒ triá»‡u chá»©ng Ä‘Ã³ thuá»™c nhiá»u hÆ¡n má»™t loáº¡i bá»‡nh.
    HÃ£y tráº£ lá»i ngáº¯n gá»n, rÃµ rÃ ng vÃ  dá»… hiá»ƒu. 
    Context cÃ³ thá»ƒ chá»©a link trang web thÃ´ng tin chi tiáº¿t vá» bá»‡nh. HÃ£y liá»‡t kÃª cÃ¡c link dáº«n tá»›i cÃ¡c trang Ä‘Ã³.

CÃ¢u há»i: {query}

ThÃ´ng tin liÃªn quan:
{context}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin trÃªn."""

    # Format context
    context_text = "\n".join([f"- {r['preview']}" for r in results])
    
    # Create prompt
    prompt = template.format(query=query, context=context_text)
    
    # Generate response
    response = model.generate_content(prompt)
    return response.text

# --- Streamlit Interface ---
st.set_page_config(page_title="Chatbot TÆ° váº¥n Y táº¿", layout="wide")

st.title("ğŸ¤– Chatbot TÆ° váº¥n Y táº¿")

# Create two columns
col1, col2 = st.columns([1, 1])

# Initialize session state for chat history and current context
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_context" not in st.session_state:
    st.session_state.current_context = []

# Load data and model
with st.spinner("Äang táº£i dá»¯ liá»‡u vÃ  model..."):
    preprocessed_documents, bm25, doc_ids = load_data()
    model = load_model()

# Chat input
query = st.text_input("ğŸ” Nháº­p cÃ¢u há»i cá»§a báº¡n vá» triá»‡u chá»©ng bá»‡nh:")

if query:
    with st.spinner("Äang tÃ¬m kiáº¿m vÃ  táº¡o cÃ¢u tráº£ lá»i..."):
        # Retrieve relevant context
        results = search_symptoms(query, preprocessed_documents, bm25, doc_ids)
        st.session_state.current_context = results
        
        if not results:
            response = "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a báº¡n."
        else:
            # Generate response using the context
            response = generate_response(query, results, model)
        
        # Add to chat history
        st.session_state.messages.append({"role": "user", "content": query})
        st.session_state.messages.append({"role": "assistant", "content": response})

# Display retrieved contexts in left column
with col1:
    st.subheader("ğŸ“š ThÃ´ng tin liÃªn quan")
    if st.session_state.current_context:
        for r in st.session_state.current_context:
            with st.expander(f"ğŸ“„ {r['doc_id']} (Score: {r['score']:.2f})"):
                st.write(r['preview'].replace("_", " "))
                st.markdown(f"[ğŸ”— Xem nguá»“n]({r['url']})")
    else:
        st.info("ChÆ°a cÃ³ thÃ´ng tin Ä‘Æ°á»£c truy xuáº¥t.")

# Display chat history in right column
with col2:
    st.subheader("ğŸ’¬ Lá»‹ch sá»­ há»™i thoáº¡i")
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(f"**Báº¡n:** {message['content']}")
        else:
            st.markdown(f"**Bot:** {message['content']}")
        st.markdown("---")
